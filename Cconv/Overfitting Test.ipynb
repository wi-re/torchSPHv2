{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5853a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import argparse\n",
    "import yaml\n",
    "from torch_geometric.nn import radius\n",
    "from torch.optim import Adam\n",
    "import torch.autograd.profiler as profiler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from rbfConv import RbfConv\n",
    "# from dataset import compressedFluidDataset, prepareData\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "def debugPrint(x):\n",
    "    frame = inspect.currentframe().f_back\n",
    "    s = inspect.getframeinfo(frame).code_context[0]\n",
    "    r = re.search(r\"\\((.*)\\)\", s).group(1)\n",
    "    print(\"{} [{}] = {}\".format(r,type(x).__name__, x))\n",
    "# %matplotlib notebook\n",
    "import copy\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import argparse\n",
    "import yaml\n",
    "from torch_geometric.nn import radius\n",
    "from torch.optim import Adam\n",
    "import torch.autograd.profiler as profiler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from rbfConv import RbfConv\n",
    "from dataset import compressedFluidDataset, prepareData\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "def debugPrint(x):\n",
    "    frame = inspect.currentframe().f_back\n",
    "    s = inspect.getframeinfo(frame).code_context[0]\n",
    "    r = re.search(r\"\\((.*)\\)\", s).group(1)\n",
    "    print(\"{} [{}] = {}\".format(r,type(x).__name__, x))\n",
    "\n",
    "\n",
    "import tomli\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "import random \n",
    "import numpy as np\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('running on: ', device)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from cutlass import *\n",
    "from rbfConv import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datautils import *\n",
    "# from sphUtils import *\n",
    "from lossFunctions import *\n",
    "\n",
    "from plotting import *\n",
    "plt.style.use('dark_background')\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca63ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-norm', '--normalized'], dest='normalized', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-e','--epochs', type=int, default=25)\n",
    "parser.add_argument('-cmap','--coordinateMapping', type=str, default='preserving')\n",
    "parser.add_argument('-w','--windowFunction', type=str, default='poly6')\n",
    "parser.add_argument('-c','--cutoff', type=int, default=1800)\n",
    "parser.add_argument('-b','--batch_size', type=int, default=2)\n",
    "parser.add_argument('--cutlassBatchSize', type=int, default=512)\n",
    "parser.add_argument('-r','--lr', type=float, default=0.01)\n",
    "parser.add_argument('--lr_decay_factor', type=float, default=0.9)\n",
    "parser.add_argument('--lr_decay_step_size', type=int, default=1)\n",
    "parser.add_argument('--weight_decay', type=float, default=0)\n",
    "parser.add_argument('-x','--rbf_x', type=str, default='linear')\n",
    "parser.add_argument('-y','--rbf_y', type=str, default='linear')\n",
    "parser.add_argument('-n','--n', type=int, default=4)\n",
    "parser.add_argument('-m','--m', type=int, default=4)\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--networkseed', type=int, default=42)\n",
    "parser.add_argument('-d','--frameDistance', type=int, default=1)\n",
    "parser.add_argument('--dataDistance', type=int, default=1)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--gpus', type=int, default=1)\n",
    "parser.add_argument('-f','--forwardLoss', type=bool, default=False)\n",
    "parser.add_argument('-v','--verbose', type=bool, default=False)\n",
    "parser.add_argument('-l','--li', type=bool, default=True)\n",
    "parser.add_argument('-a','--activation', type=str, default='relu')\n",
    "parser.add_argument('--arch', type=str, default='32 64 64 3')\n",
    "parser.add_argument('--limitData', type=int, default=-1)\n",
    "parser.add_argument('--iterations', type=int, default=1000)\n",
    "parser.add_argument('-u', '--maxUnroll', type=int, default=10)\n",
    "parser.add_argument('--minUnroll', type=int, default=2)\n",
    "parser.add_argument('-augj', '--augmentJitter', type=bool, default=True)\n",
    "parser.add_argument('-j', '--jitterAmount', type=float, default=0.01)\n",
    "parser.add_argument('-augr', '--augmentAngle', type=bool, default=True)\n",
    "parser.add_argument('-adjust', '--adjustForFrameDistance', type = bool, default = True)\n",
    "parser.add_argument('-netArch', '--network', type=str, default='default')\n",
    "parser.add_argument('-norm', '--normalized', type=bool, default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f25b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94b62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"--verbose TRUE --frameDistance 16 -n 4 -m 4 --arch '32 64 64 3' -x 'rbf cubic_spline' -y 'rbf cubic_spline' -w None -cmap cartesian --epochs 25 --networkseed 3 -netArch default\"\n",
    "a = shlex.split(a)\n",
    "args = parser.parse_args(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a32687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting all rng seeds to 42\n",
      "Available cuda devices: 0\n",
      "Running on Device cpu\n",
      "Parsing data in ../export\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.verbose:\n",
    "    print('Setting all rng seeds to %d' % args.seed)\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "if args.verbose:\n",
    "    print('Available cuda devices:', torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if args.verbose:\n",
    "    print('Running on Device %s' % device)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "from cutlass import *\n",
    "from rbfConv import *\n",
    "from datautils import *\n",
    "from plotting import *\n",
    "\n",
    "# Use dark theme\n",
    "plt.style.use('dark_background')\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from rbfNet import *\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "if args.verbose:\n",
    "    print('Parsing data in ../export')\n",
    "basePath = '../export'\n",
    "basePath = os.path.expanduser('~/dev/datasets/generative2D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156e6e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "\t 0 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-49-55.hdf5\n",
      "\t 1 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-59-11.hdf5\n",
      "\t 2 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-08-03.hdf5\n",
      "\t 3 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-26-38.hdf5\n",
      "\t 4 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-13-28.hdf5\n",
      "\t 5 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-35-51.hdf5\n",
      "\t 6 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-03-59.hdf5\n",
      "\t 7 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-17-15.hdf5\n",
      "\t 8 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-31-08.hdf5\n",
      "\t 9 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-21-57.hdf5\n",
      "\t 10 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-54-35.hdf5\n",
      "\t 11 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-12-40.hdf5\n",
      "\t 12 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-18-15.hdf5\n",
      "\t 13 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-45-14.hdf5\n",
      "\t 14 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-08-43.hdf5\n",
      "\t 15 /home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-40-31.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# basePath = '~/dev/datasets/WBCSPH2Dc/train'\n",
    "# basePath = os.path.expanduser(basePath)\n",
    "\n",
    "# simulationFiles = [basePath + '/' + f for f in os.listdir(basePath) if f.endswith('.zst')]\n",
    "trainingFiles = [basePath + '/train/' + f for f in os.listdir(basePath + '/train/') if f.endswith('.hdf5')]\n",
    "# validationFiles = [basePath + '/valid/' + f for f in os.listdir(basePath + '/valid/') if f.endswith('.hdf5')]\n",
    "# for i, c in enumerate(simulationFiles):\n",
    "#     print(i ,c)\n",
    "#     \n",
    "# simulationFiles  = [simulationFiles[0]]\n",
    "# simulationFiles = simulationFiles[:1]\n",
    "\n",
    "training = []\n",
    "validation = []\n",
    "testing = []\n",
    "\n",
    "    \n",
    "    # for s in simulationFiles:    \n",
    "#     _, train, valid, test = splitFileZSTD(s, split = True, limitRollOut = False, skip = 0, cutoff = 1800, distance = 1)\n",
    "#     training.append((s,train))\n",
    "#     validation.append((s,valid))\n",
    "#     testing.append((s,test))\n",
    "# debugPrint(training)\n",
    "\n",
    "# simulationFiles = [basePath + '/' + f for f in os.listdir(basePath) if f.endswith('.hdf5')]\n",
    "\n",
    "if args.limitData > 0:\n",
    "    files = []\n",
    "    for i in range(max(len(trainingFiles), args.limitData)):\n",
    "        files.append(trainingFiles[i])\n",
    "    simulationFiles = files\n",
    "# simulationFiles = [simulationFiles[0]]\n",
    "if args.verbose:\n",
    "    print('Input files:')\n",
    "    for i, c in enumerate(trainingFiles):\n",
    "        print('\\t', i ,c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed4beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data into datasets:\n",
      "training [list] = [('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-49-55.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-59-11.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-08-03.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-26-38.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-13-28.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-35-51.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-03-59.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-17-15.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-31-08.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-21-57.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-54-35.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-12-40.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-18-15.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-45-14.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_17-08-43.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32))), ('/home/winchenbach/dev/datasets/generative2D/train/generative_16 - 2023-03-10_16-40-31.hdf5', (array([  16,   17,   18, ..., 3034, 3035, 3036]), array([8, 8, 8, ..., 3, 2, 1], dtype=int32)))]\n",
      "validation [list] = []\n",
      "testing [list] = []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training = []\n",
    "validation = []\n",
    "testing = []\n",
    "\n",
    "for s in trainingFiles:\n",
    "    f, s, u = splitFile(s, split = False, cutoff = -args.frameDistance * args.maxUnroll, skip = args.frameDistance if args.adjustForFrameDistance else 0)\n",
    "    training.append((f, (s,u)))\n",
    "# for s in tqdm(validationFiles):\n",
    "#     f, s, u = splitFile(s, split = False, cutoff = -4, skip = 0)\n",
    "#     validation.append((f, (s,u)))\n",
    "    \n",
    "if args.verbose:\n",
    "    print('Processed data into datasets:')\n",
    "    debugPrint(training)\n",
    "    debugPrint(validation)\n",
    "    debugPrint(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16dc1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data loaders\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = args.batch_size\n",
    "\n",
    "if args.verbose:\n",
    "    print('Setting up data loaders')\n",
    "train_ds = datasetLoader(training)\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size = batch_size).batch_sampler\n",
    "\n",
    "# validation_ds = datasetLoader(validation)\n",
    "# validation_dataloader = DataLoader(validation_ds, shuffle=True, batch_size = batch_size).batch_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0a028ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up network parameters:\n",
      "Network Hyperparameters:\n",
      "[n x m]: [4x4]\n",
      "[rbf_x x rbf_y]: [rbf cubic_splinexrbf cubic_spline]\n",
      "Mapping: cartesian\n",
      "window function: None\n",
      "activation function: relu\n",
      "initial learning rate:  0.01\n",
      "Training for 25 epochs\n",
      "Rollout limit (if applicable): 8\n",
      "Training with frame offset of 16\n",
      "Network architecture 32 64 64 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.verbose:\n",
    "    print('Setting up network parameters:')\n",
    "fileName, frameIndex, maxRollout = train_ds[len(train_ds)//2]\n",
    "attributes, inputData, groundTruthData = loadFrame(fileName, frameIndex, 1 + np.arange(1))\n",
    "\n",
    "fluidPositions, boundaryPositions, fluidFeatures, boundaryFeatures = constructFluidFeatures(attributes, inputData)\n",
    "\n",
    "n = args.n\n",
    "m = args.m\n",
    "coordinateMapping = args.coordinateMapping\n",
    "windowFn = getWindowFunction(args.windowFunction)\n",
    "rbf_x = args.rbf_x\n",
    "rbf_y = args.rbf_y\n",
    "initialLR = args.lr\n",
    "maxRollOut = 10\n",
    "epochs = args.epochs\n",
    "frameDistance = args.frameDistance\n",
    "\n",
    "if args.verbose:\n",
    "    print('Network Hyperparameters:')\n",
    "    print('[n x m]: [%dx%d]'% (n, m))\n",
    "    print('[rbf_x x rbf_y]: [%sx%s]'% (rbf_x, rbf_y))\n",
    "    print('Mapping:', args.coordinateMapping)\n",
    "    print('window function:', args.windowFunction)\n",
    "    print('activation function:', args.activation)\n",
    "    print('initial learning rate: ', initialLR)\n",
    "    print('Training for %d epochs' % epochs)\n",
    "    print('Rollout limit (if applicable):', maxRollout)\n",
    "    print('Training with frame offset of', frameDistance)\n",
    "    print('Network architecture', args.arch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd40d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Network\n",
      "Number of parameters 181414\n"
     ]
    }
   ],
   "source": [
    "\n",
    "widths = args.arch.strip().split(' ')\n",
    "layers = [int(s) for s in widths]\n",
    "# debugPrint(layers)\n",
    "if args.verbose:\n",
    "    print('Building Network')\n",
    "\n",
    "random.seed(args.networkseed)\n",
    "torch.manual_seed(args.networkseed)\n",
    "torch.cuda.manual_seed(args.networkseed)\n",
    "np.random.seed(args.networkseed)\n",
    "\n",
    "\n",
    "\n",
    "model = None\n",
    "if args.network == 'default':\n",
    "    model = RbfNet(fluidFeatures.shape[1], boundaryFeatures.shape[1], layers = layers, coordinateMapping = coordinateMapping, n = n, m = m, windowFn = windowFn, rbf_x = rbf_x, rbf_y = rbf_y, batchSize = args.cutlassBatchSize, normalized = args.normalized)\n",
    "if args.network == 'split':\n",
    "    model = RbfSplitNet(fluidFeatures.shape[1], boundaryFeatures.shape[1], layers = layers, coordinateMapping = coordinateMapping, n = n, m = m, windowFn = windowFn, rbf_x = rbf_x, rbf_y = rbf_y, batchSize = args.cutlassBatchSize, normalized = args.normalized)\n",
    "if args.network == 'interleaved':\n",
    "    model = RbfInterleaveNet(fluidFeatures.shape[1], boundaryFeatures.shape[1], layers = layers, coordinateMapping = coordinateMapping, n = n, m = m, windowFn = windowFn, rbf_x = rbf_x, rbf_y = rbf_y, batchSize = args.cutlassBatchSize, normalized = args.normalized)\n",
    "if args.network == 'input':\n",
    "    model = RbfInputNet(fluidFeatures.shape[1], boundaryFeatures.shape[1], layers = layers, coordinateMapping = coordinateMapping, n = n, m = m, windowFn = windowFn, rbf_x = rbf_x, rbf_y = rbf_y, batchSize = args.cutlassBatchSize, normalized = args.normalized)\n",
    "if args.network == 'output':\n",
    "    model = RbfOutputNet(fluidFeatures.shape[1], boundaryFeatures.shape[1], layers = layers, coordinateMapping = coordinateMapping, n = n, m = m, windowFn = windowFn, rbf_x = rbf_x, rbf_y = rbf_y, batchSize = args.cutlassBatchSize, normalized = args.normalized)\n",
    "\n",
    "\n",
    "lr = initialLR\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "if args.verbose:\n",
    "    print('Number of parameters', count_parameters(model))\n",
    "    \n",
    "    \n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "\n",
    "hyperParameterDict = {}\n",
    "hyperParameterDict['n'] = n\n",
    "hyperParameterDict['m'] = m\n",
    "hyperParameterDict['coordinateMapping'] = coordinateMapping\n",
    "hyperParameterDict['rbf_x'] = rbf_x\n",
    "hyperParameterDict['rbf_y'] = rbf_y\n",
    "hyperParameterDict['windowFunction'] =  args.windowFunction\n",
    "hyperParameterDict['liLoss'] = 'yes' if args.li else 'no'\n",
    "hyperParameterDict['initialLR'] = initialLR\n",
    "hyperParameterDict['maxRollOut'] = maxRollOut\n",
    "hyperParameterDict['epochs'] = epochs\n",
    "hyperParameterDict['frameDistance'] = frameDistance\n",
    "hyperParameterDict['dataDistance'] = args.dataDistance\n",
    "hyperParameterDict['parameters'] =  count_parameters(model)\n",
    "hyperParameterDict['cutoff'] =  args.cutoff\n",
    "hyperParameterDict['dataLimit'] =  args.limitData \n",
    "hyperParameterDict['arch'] =  args.arch\n",
    "hyperParameterDict['seed'] =  args.seed\n",
    "hyperParameterDict['minUnroll'] =  args.minUnroll\n",
    "hyperParameterDict['maxUnroll'] =  args.maxUnroll\n",
    "hyperParameterDict['augmentAngle'] =  args.augmentAngle\n",
    "hyperParameterDict['augmentJitter'] =  args.augmentJitter\n",
    "hyperParameterDict['jitterAmount'] =  args.jitterAmount\n",
    "hyperParameterDict['networkSeed'] =  args.networkseed\n",
    "hyperParameterDict['network'] = args.network\n",
    "hyperParameterDict['normalized'] = args.normalized\n",
    "hyperParameterDict['adjustForFrameDistance'] = args.adjustForFrameDistance\n",
    "lr = initialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e674fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output to ./trainingData/default - n=[ 4, 4] rbf=[rbf cubic_spline,rbf cubic_spline] map = cartesian window = None d = 16 e = 25 arch 32 64 64 3 distance = 16 - 2023-06-06_20-58-13 seed 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "networkPrefix = args.network\n",
    "\n",
    "exportString = '%s - n=[%2d,%2d] rbf=[%s,%s] map = %s window = %s d = %2d e = %2d arch %s distance = %2d - %s seed %s' % (networkPrefix, hyperParameterDict['n'], hyperParameterDict['m'], hyperParameterDict['rbf_x'], hyperParameterDict['rbf_y'], hyperParameterDict['coordinateMapping'], args.windowFunction, hyperParameterDict['frameDistance'], hyperParameterDict['epochs'], args.arch, frameDistance, timestamp, args.networkseed)\n",
    "\n",
    "shortLabel = '%14s [%14s] - %s -> [%16s, %16s] x [%2d, %2d] @ %2s ' % (hyperParameterDict['windowFunction'], hyperParameterDict['arch'], hyperParameterDict['coordinateMapping'], hyperParameterDict['rbf_x'], hyperParameterDict['rbf_y'], hyperParameterDict['n'], hyperParameterDict['m'],hyperParameterDict['networkSeed'])\n",
    "# print(shortLabel)\n",
    "\n",
    "# exit()\n",
    "# if args.gpus == 1:\n",
    "\n",
    "#     debugPrint(hyperParameterDict)\n",
    "# if args.gpus == 1:\n",
    "#     debugPrint(exportString)\n",
    "if args.verbose:\n",
    "    print('Writing output to ./trainingData/%s' % exportString)\n",
    "\n",
    "# exportPath = './trainingData/%s - %s.hdf5' %(self.config['export']['prefix'], timestamp)\n",
    "if not os.path.exists('./trainingData/%s' % exportString):\n",
    "    os.makedirs('./trainingData/%s' % exportString)\n",
    "# self.outFile = h5py.File(self.exportPath,'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processBatch(model, device, li, e, unroll, train_ds, bdata, frameDistance, augmentAngle = False, augmentJitter = False, jitterAmount = 0.01, adjustForFrameDistance = True):\n",
    "    with record_function(\"process batch\"): \n",
    "        fluidPositions, boundaryPositions, fluidFeatures, boundaryFeatures, fluidGravity, fluidBatches, boundaryBatches, groundTruths, attributes = \\\n",
    "            loadBatch(train_ds, bdata, constructFluidFeatures, unroll, frameDistance, augmentAngle = augmentAngle, augmentJitter = augmentJitter, jitterAmount = jitterAmount, adjustForFrameDistance = adjustForFrameDistance)    \n",
    "\n",
    "\n",
    "        predictedPositions = fluidPositions.to(device)\n",
    "        predictedVelocity = fluidFeatures[:,1:3].to(device)\n",
    "\n",
    "        bLosses = []\n",
    "        boundaryPositions = boundaryPositions.to(device)\n",
    "        fluidFeatures = fluidFeatures.to(device)\n",
    "        boundaryFeatures = boundaryFeatures.to(device)\n",
    "        fluidBatches = fluidBatches.to(device)\n",
    "        boundaryBatches = boundaryBatches.to(device)\n",
    "\n",
    "        gravity = torch.zeros_like(predictedVelocity)\n",
    "        gravity = fluidGravity[:,:2].to(device)\n",
    "        \n",
    "    #     gravity[:,1] = -9.81\n",
    "\n",
    "        for u in range(unroll):\n",
    "            with record_function(\"prcess batch[unroll]\"): \n",
    "    #         loss, predictedPositions, predictedVelocity = runNetwork(fluidPositions.to(device), inputData['fluidVelocity'].to(device), attributes['dt'], frameDistance, gravity, fluidFeatures, boundaryPositions.to(device), boundaryFeatures.to(device), groundTruths[0], model, None, None, True)\n",
    "                loss, predictedPositions, predictedVelocity = runNetwork(predictedPositions, predictedVelocity, attributes[0], frameDistance, gravity, fluidFeatures, boundaryPositions, boundaryFeatures, groundTruths[u], model, fluidBatches, boundaryBatches, li)\n",
    "\n",
    "                batchedLoss = []\n",
    "                for i in range(len(bdata)):\n",
    "                    L = loss[fluidBatches == i]\n",
    "                    Lterms = (torch.mean(L), torch.max(torch.abs(L)), torch.min(torch.abs(L)), torch.std(L))            \n",
    "                    batchedLoss.append(torch.hstack(Lterms))\n",
    "                batchedLoss = torch.vstack(batchedLoss).unsqueeze(0)\n",
    "                bLosses.append(batchedLoss)\n",
    "\n",
    "        bLosses = torch.vstack(bLosses)\n",
    "        maxLosses = torch.max(bLosses[:,:,1], dim = 0)[0]\n",
    "        minLosses = torch.min(bLosses[:,:,2], dim = 0)[0]\n",
    "        meanLosses = torch.mean(bLosses[:,:,0], dim = 0)\n",
    "        stdLosses = torch.mean(bLosses[:,:,3], dim = 0)\n",
    "\n",
    "\n",
    "        del predictedPositions, predictedVelocity, boundaryPositions, fluidFeatures, boundaryFeatures, fluidBatches, boundaryBatches\n",
    "\n",
    "        bLosses = bLosses.transpose(0,1)\n",
    "\n",
    "        return bLosses, meanLosses, minLosses, maxLosses, stdLosses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ca3e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processDataLoaderIter(iterations, e, rollout, ds, dataLoader, dataIter, model, optimizer, train = True, prefix = '', augmentAngle = False, augmentJitter = False, jitterAmount = 0.01):\n",
    "    with record_function(\"prcess data loader\"): \n",
    "        losses = []\n",
    "        batchIndices = []\n",
    "\n",
    "        if train:\n",
    "            model.train(True)\n",
    "        else:\n",
    "            model.train(False)\n",
    "\n",
    "        i = 0\n",
    "        for b in (pbl := tqdm(range(iterations), leave=False)):\n",
    "            try:\n",
    "                bdata = next(dataIter)\n",
    "            except:\n",
    "                dataIter = iter(dataLoader)\n",
    "                bdata = next(dataIter)\n",
    "                \n",
    "            with record_function(\"prcess data loader[batch]\"): \n",
    "                if train:\n",
    "                    optimizer.zero_grad()\n",
    "                batchLosses, meanLosses, minLosses, maxLosses, stdLosses = processBatch(model, device, True, e, rollout, ds, bdata, frameDistance, augmentAngle, augmentJitter, jitterAmount, adjustForFrameDistance = args.adjustForFrameDistance)\n",
    "                # print(torch.max(model.ni))\n",
    "                \n",
    "                batchIndices.append(np.array(bdata))\n",
    "                losses.append(batchLosses.detach().cpu().numpy())\n",
    "\n",
    "                with record_function(\"prcess data loader[batch] - backward\"): \n",
    "                    sumLosses = torch.mean(batchLosses[:,:,0]) #+ torch.mean(batchLosses[:,:,1])\n",
    "                    if train:\n",
    "                        sumLosses.backward()\n",
    "                        optimizer.step()\n",
    "                lossString = np.array2string(torch.mean(batchLosses[:,:,0],dim=0).detach().cpu().numpy(), formatter={'float_kind':lambda x: \"%.2e\" % x})\n",
    "                batchString = str(np.array2string(np.array(bdata), formatter={'float_kind':lambda x: \"%.2f\" % x, 'int':lambda x:'%04d' % x}))\n",
    "\n",
    "#                 with portalocker.Lock('README.md', flags = 0x2, timeout = None):\n",
    "                pbl.set_description('%8s[gpu %d]: %3d [%1d] @ %1.1e: :  %s -> %.2e' %(prefix, args.gpu, e, rollout, lr, batchString, sumLosses.detach().cpu().numpy()))\n",
    "                pbl.update()\n",
    "                if prefix == 'training':\n",
    "                    # pb.set_description('[gpu %d] Learning: %1.4e Validation: %1.4e' %(args.gpu, np.mean(np.mean(np.vstack(losses)[:,:,0], axis = 1)), 0))\n",
    "                    pb.set_description('[gpu %d] %90s - Learning: %1.4e' %(args.gpu, shortLabel, np.mean(np.mean(np.vstack(losses)[:,:,0], axis = 1))))\n",
    "                if prefix == 'validation':\n",
    "                    pb.set_description('[gpu %d] Learning: %1.4e Validation: %1.4e' %(args.gpu, trainLoss, np.mean(np.mean(np.vstack(losses)[:,:,0], axis = 1))))\n",
    "                pb.update()\n",
    "#                 i = i + 1\n",
    "#                 if i > 100:\n",
    "#                     break\n",
    "        bIndices  = np.hstack(batchIndices)\n",
    "        losses = np.vstack(losses)\n",
    "\n",
    "        # idx = np.argsort(bIndices)\n",
    "        # bIndices = bIndices[idx]\n",
    "        # losses = losses[idx]\n",
    "\n",
    "        epochLoss = losses\n",
    "        return epochLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "299e36dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055f07ee366240cf9f85abda27697c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105d8966d874a4aa92f435f4039c80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     unroll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(args\u001b[38;5;241m.\u001b[39mminUnroll, \u001b[38;5;28mmin\u001b[39m(epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mmaxUnroll))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# trainingEpochLoss = processDataLoaderIter(args.iterations, epoch, epoch // 2 + 1, train_ds, train_dataloader, train_iter, model, optimizer, True, prefix = 'training', augmentAngle=args.argumentAngle, augmentJitter=args.augmentJitter, jitterAmount=args.jitterAmount)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     trainingEpochLoss \u001b[38;5;241m=\u001b[39m \u001b[43mprocessDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentAngle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentAngle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentJitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentJitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitterAmount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjitterAmount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     trainingEpochLoss = processDataLoader(epoch,unroll, train_ds, train_dataloader, model, optimizer, True, prefix = 'training')\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     trainingEpochLosses\u001b[38;5;241m.\u001b[39mappend(trainingEpochLoss)\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mprocessDataLoaderIter\u001b[0;34m(iterations, e, rollout, ds, dataLoader, dataIter, model, optimizer, train, prefix, augmentAngle, augmentJitter, jitterAmount)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m batchLosses, meanLosses, minLosses, maxLosses, stdLosses \u001b[38;5;241m=\u001b[39m \u001b[43mprocessBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeDistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentAngle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentJitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitterAmount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjustForFrameDistance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjustForFrameDistance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(torch.max(model.ni))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m batchIndices\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(bdata))\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/rbfNet.py:779\u001b[0m, in \u001b[0;36mprocessBatch\u001b[0;34m(model, device, li, e, unroll, train_ds, bdata, frameDistance, augmentAngle, augmentJitter, jitterAmount, adjustForFrameDistance)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(unroll):\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprcess batch[unroll]\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m#         loss, predictedPositions, predictedVelocity = runNetwork(fluidPositions.to(device), inputData['fluidVelocity'].to(device), attributes['dt'], frameDistance, gravity, fluidFeatures, boundaryPositions.to(device), boundaryFeatures.to(device), groundTruths[0], model, None, None, True)\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m             loss, predictedPositions, predictedVelocity \u001b[38;5;241m=\u001b[39m \u001b[43mrunNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictedPositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictedVelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeDistance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgravity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryPositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroundTruths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidBatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryBatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m             batchedLoss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    782\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bdata)):\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/rbfNet.py:714\u001b[0m, in \u001b[0;36mrunNetwork\u001b[0;34m(initialPosition, initialVelocity, attributes, frameDistance, gravity, fluidFeatures, boundaryPositions, boundaryFeatures, groundTruth, model, fluidBatches, boundaryBatches, li)\u001b[0m\n\u001b[1;32m    707\u001b[0m fluidFeatures \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack((fluidFeatures[:,\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;28;01mNone\u001b[39;00m], vel2, fluidFeatures[:,\u001b[38;5;241m3\u001b[39m:]))\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# if verbose:\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m#     print('calling network with' )\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m#     print('d', d)\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m#     print('vel2', vel2[:4])\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m#     print('pos2', pos2[:4])\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m#     print('fluidFeatures', fluidFeatures[:4])\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryPositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidBatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundaryBatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m predictedVelocity \u001b[38;5;241m=\u001b[39m (pos2 \u001b[38;5;241m+\u001b[39m predictions[:,:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m initialPosition) \u001b[38;5;241m/\u001b[39m (frameDistance \u001b[38;5;241m*\u001b[39m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    717\u001b[0m predictedPositions \u001b[38;5;241m=\u001b[39m pos2 \u001b[38;5;241m+\u001b[39m predictions[:,:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/rbfNet.py:140\u001b[0m, in \u001b[0;36mRbfNet.forward\u001b[0;34m(self, fluidPositions, boundaryPositions, fluidFeatures, boundaryFeatures, attributes, fluidBatches, boundaryBatches)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,layers):\n\u001b[1;32m    138\u001b[0m     ansc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(ans)\n\u001b[0;32m--> 140\u001b[0m     ansConv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mansc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mansc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidEdgeIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfluidEdgeLengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     ansDense \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m](ansc)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m ans\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ansConv\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/rbfConv.py:348\u001b[0m, in \u001b[0;36mRbfConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n\u001b[1;32m    340\u001b[0m                 edge_attr: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size: Size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;66;03m# print('x', x[0].shape, x)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[38;5;66;03m# out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\u001b[39;00m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate2\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m#         print('out: ', out.shape, out)\u001b[39;00m\n\u001b[1;32m    353\u001b[0m         x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/rbfConv.py:451\u001b[0m, in \u001b[0;36mRbfConv.propagate2\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     cubePositions \u001b[38;5;241m=\u001b[39m mapToSpherePreserving(positions)\n\u001b[1;32m    448\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack((cubePositions[:,\u001b[38;5;241m0\u001b[39m],cubePositions[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpi))\u001b[38;5;241m.\u001b[39mmT\n\u001b[0;32m--> 451\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrbfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiodic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizeInterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    457\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m~/dev/torchSPHv2/Cconv/cutlass.py:357\u001b[0m, in \u001b[0;36mcutlass.forward\u001b[0;34m(ctx, edge_index, features_i, features_j, edge_attr, edge_weights, weight, dim_size, dim, size, rbfs, periodic, forwardBatchSize, backwardBatchSize, normalized)\u001b[0m\n\u001b[1;32m    355\u001b[0m         conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnu, nv, uvio,ni -> no\u001b[39m\u001b[38;5;124m'\u001b[39m,u,v,weight, x_j[batch]) \u001b[38;5;241m*\u001b[39m normalizationFactor[:,\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m         conv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnu, nv, uvio,ni -> no\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# print('u', u.dtype, u.shape)\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# print('v', v.dtype, v.shape)\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# print('weight', weight.dtype, weight.shape)\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# print('x_j', x_j.dtype, x_j.shape)\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# print('x_j[batch]', x_j[batch].dtype, x_j[batch].shape)\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# print('u', u.dtype, u.shape)\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m u,v\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.9/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training = {}\n",
    "validation = {}\n",
    "testing = {}\n",
    "\n",
    "trainLoss = 0\n",
    "validationLoss = 0\n",
    "train_iter = iter(train_dataloader)\n",
    "\n",
    "trainingEpochLosses = []\n",
    "trainingEpochLosses2 = []\n",
    "validationLosses = []\n",
    "\n",
    "unroll = 2\n",
    "\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "# if args.verbose:\n",
    "    # print('Start of training')\n",
    "\n",
    "pb = tqdm(range(epochs * args.iterations))\n",
    "# pb.reset(total=len(train_dataloader))\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "\n",
    "    unroll = max(args.minUnroll, min(epoch // 2 + 1, args.maxUnroll))\n",
    "    # trainingEpochLoss = processDataLoaderIter(args.iterations, epoch, epoch // 2 + 1, train_ds, train_dataloader, train_iter, model, optimizer, True, prefix = 'training', augmentAngle=args.argumentAngle, augmentJitter=args.augmentJitter, jitterAmount=args.jitterAmount)\n",
    "    trainingEpochLoss = processDataLoaderIter(args.iterations, epoch, unroll, train_ds, train_dataloader, train_iter, model, optimizer, True, prefix = 'training', augmentAngle=args.augmentAngle, augmentJitter=args.augmentJitter, jitterAmount=args.jitterAmount)\n",
    "\n",
    "#     trainingEpochLoss = processDataLoader(epoch,unroll, train_ds, train_dataloader, model, optimizer, True, prefix = 'training')\n",
    "    trainingEpochLosses.append(trainingEpochLoss)\n",
    "    # torch.save(model.state_dict(), './trainingData/%s/model_%03d.torch' % (exportString, epoch))\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        lr = lr * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "    torch.save(model.state_dict(), './trainingData/%s/model_%03d.torch' % (exportString, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45588fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
